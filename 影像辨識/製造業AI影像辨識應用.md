- AI影像模型訓練-以CVAT為例
    - 角色權限設定-以BailAI平台/慧演科技為例
        - 超級使用者：小組的帳號管理。
        - 組長：模型訓練及推論、模型增刪改查。
        - 組員：樣本標註。
    - 步驟
        1. 資料上傳(upload)：打包PNG、JPG到ZIP，直接把資料集上傳至CVAT平台。[CVAT](https://docs.cvat.ai/docs/)。
        2. 資料標記(labeling)：透過Project > Task > Job不同大小層級管理標註、小組狀態。標註介面可操作分類、偵測、分割、影片的標記。
        3. 建立專案資料卡，再建立訓練任務：編輯參數，調整訓練任務(20次 ~ 100次)、模型(如YOLOv5)、資料集。
        4. 訓練過程 ~ 模型推論：隨時查看訓練指標，如訓練進度、硬體資源、精度(每個Epoc的結果)，執行模型的圖形驗證(雲端推論)，部署完成的模型。
        5. 關閉VM。

- 影像分類(image classification)
    - 原理：依據深度視覺網路(卷積神經網絡 CNN)、機器視覺技術的架構，把圖片分解成許多像素，在隱藏層逐層提取、組合影像特徵，最終輸出結果。
    - 概念：1.提取特徵、2.建立數學模型、3.訓練模型。
    - 應用：檢體病變，自動駕駛，人臉辨識，社群標記。
    - 實作：使用tag標註。
    - 常見資料集格式：CVAT、COCO、YOLO。(匯出格式)
- 物件偵測(object localization)
    - 定義：在圖像或影像中識別多個對象，並定位特定物體。
    - 概念：結合分類(classification)與定位(localization)。使用滑動視窗(sliding window)比對物件，再以邊界框(bounding box)框出物件的矩形位置和大小(x,y,w,h)。
    - 應用：提供攝影機、光達、雷達等訊號源的感測能力。
        - 車輛駕駛ADAS：感測(車前鏡頭+毫米波雷達+超聲波雷達+光達) -> 分析/決策(ECU) -> 執行(驅動系統、制動系統、轉向系統)。
        - 安全監控系統：CCTV -> 邊緣運算(WiFi、LTE、Router -> 雲計算 -> 集中管理系統)
        - 運動產業-動作姿態、球體軌跡。
    - 實作：使用矩形框標註。
    - 常見資料集格式：YOLO。(匯出格式)
- 影像分割(segmantation)
    - 定義：將相似特性顏色、強度、紋理的像素，區分到同一區域。
    - 概念：語意分割(semantic)、實例分割(instance)。提供像素級影像辨識結果，增強AI系統感知能力和應用。
    - 應用：研究瑕疵來源和細節。
        - 醫學影像的病變區域判定。
        - 遙測影像的土地利用分類、演變。
        - 檢測缺陷的紋理狀態：產品檢測 -> 語意分割 -> 資料紀錄 -> 專家系統調參 -> 產品檢測。
    - 實作：使用polygon標註(根據需求調整精細度，也可透過輔助工具)。
- 影片標註
    - 概念：定義標註目標(對象+標註方式-邊界框/多邊形/關鍵點)、影像分解成楨進行自動化標註(設置關鍵楨/起始楨/中間楨->自動內插)
    - 優勢
        - 增強模型訓練效果：加入時間連續、豐富資料集
        - 提高標註效率：批量、自動追蹤
        - 提高標註精度：上下文參考、動態場景
        - 多樣化應用場景
        - 多類型標註
        - 適用長期監控：行為分析、趨勢預測
        - 改善標註品質：一致性檢查
    - 應用
        - 紙罐工廠自動化品檢：加入輪轉機，標註產品的各種瑕疵。
        - 幫浦或KVM工序SOP：生產人員是否按照SOP操作。
    - 實作：上傳影片，查閱影格，標註特定物件。完成後自動下載檔案夾。
        - 用關鍵楨自動生成移動(對非勻速運動要逐楨手動調整)
        - 用AI工具輔助，如演算法TransT(辨識速度較慢)。
        - 用OpenCV追蹤模式，如trackerMIL(精度較低要逐楨手動調整)。
- 自動標註(annotate)
    - 概念：利用預訓練模型，完成圖像或影像的檢測和標註，也可小量試迭代優化此模型(半自動化)，後續節省標註的人工成本。
    - 實作版本
        - CVAT線上版-免費版用戶：單張自動推理標註。
        - CVAT付費版-免費版用戶：批次自動標註。
        - CVAT自架/桌機-部署客製化預模型並迭代優化此模型。
    - 實作
        - 預設預訓練模型
            - 屬性人臉檢測：Face Detection 0205 + Emotions Recognition Retail 0003 + Age Gender Recognition Retail 0013。
            - RetinaNet R101
            - 文本檢測：MobileNetV2、OpenVINO Text Detection 004。
            - YOLO v3
            - YOLO v7：COCO標記80種類別。
        - 導入預訓練模型：URL + API。
            - 可支援hugface的模型
            - 可支援robflow的模型：金屬瑕疵、PCB板瑕疵
        - 類別標籤名稱設定，可映射到預訓練模型的名稱，無須相同名稱
        - 使用模型標註：AI Tool > Detector 選取預訓練模型，自動偵測指定物件。
- 資料擴增(data augmentation)
    - 概念：加強模型的泛化能力，避免模型過擬合、提高模型性能，降低收資料集成本。
    - 技術
        - 基本：資料旋轉、平移、縮放、翻轉、剪裁、斜切、色彩調整(亮度、對比度、飽和度)。
        - 進階：高斯模糊、噪聲增加、cutout、mixup、合成擴增、挖洞、填色、填料、馬賽克拼貼。
        - 生成式：生成對抗網路(GAN)、立體渲染(3D rendind)/數位孿生(dirital twins)、AI生成內容(AIGC)。
    - 實作：roboflow
        1. 建立新專案，上傳原始圖片。
        2. 標註物件，存入資料集。
        3. Generate > Add Augmentation step (擴大空間) > continue > 資料倍數(免費3倍，付費50倍)
    - 注意：重要特徵的色彩、形狀不可被變更太多。
    - 常見格式：YOLO、COCO。

- 物件偵測演算法
    - 受惠於深度學習的發展。
    - 二階段演算法(two stage)：分成物件定位(候選框)、物件偵測兩階段。推論慢，較精確。如R-CNN。
    - 一階段演算法(one stage)：同時完成物件定位、物件偵測。推論快，精度差。適用於24 FPS的影像。如YOLO v5/v8偵測球賽。

- 常見物件偵測算法
    - 卷積神經網路CNN
        - 架構：convolution、pooling、fully connected、output (機率最大)。
        - 概念：使用滑動視窗，檢測物件，都需要丟入CNN判斷相當耗時。
    - 區域卷積神經網路R-CNN
        - 架構：輸入圖片，產生2000個預選框，生成候選框之後，resize大小至丟入CNN判斷有無物件。
        - 概念：候選框生成、特徵提取、類別判斷、位置精修。
            - 以非極大值抑制(non-maximum supression, NMS)，以ROU閥值選出最大的預選框，篩除冗餘框。
        - 相關衍生
            - fast R-CNN：以Selective Search產生候選框，對特徵萃取直接映射到原圖，再透過ROI pooling技術把該區特徵resize，丟入全連接層分類。
                - 只使用一個模型。節省運算時間，提升mAP，特徵緩存無須磁碟儲存。
            - faster R-CNN：以Reginal Proposal Network (RPN)產生候選框，之後與fast R-CNN相同。
                - RPN是端到端的偵測模型，使用來自CNN特徵圖的k個anchor box，對特徵向量預測出4k預選框，最後取得得分、邊界框。
                - 解決預選框產出效率，實現端到端的網模模型。硬體需求增加。
            - mask R-CNN
                - 概念：使用全卷積網路(FCN)，物件偵測+輪廓分割。
                - 應用
                    - Detectron
                    - Detectron2：用PyTorch
    - YOLO
        - 架構：resize -> CNN -> NMS。物件定位/辨識是採迴歸問題解出，預測長寬、中心點，得出辨識機率。
        - 特性
            - 高速性能：可處理45 FPS (目前GPU強大可達200 FPS以上)。
            - 全圖視角：減少背景誤差。區隔物件和背景。
            - 高泛用性：簡化版模型也可用。硬體裝置要求不高。
            - 單一架構：採統一的CNN，同時預測邊界框、類別機率。
            - 高準確性：YOLO v5/v8可相當於二階段偵測(不包括YOLO v1/v2)。
        - 應用
            - 瑕疵檢測、人員移動之即時監控、影像串流。

- 模型訓練
    - 步驟
        1. 資料收集與前處理：標註、清洗、擴增。
            - 公開資料集：COCO、VOC、瑕疵檢測、自駕車、姿態。
        2. 模型選擇與構建：深度學習框架(TensorFlow業界, Pytorch學術)、架構模型/一階二階(Faster R-CNN、YOLO、SSD)。
            - 演算法種類：分類任務(CNN)、偵測任務(YOLO、R-CNN、SSD)、分割任務(Mask RCNN)。
            - 實時應用是速度取向(如犯罪預防)，高精度需求是準確性高(如醫學)。
        3. 模型訓練-設定參數、超參數
            - 學習率：是權重更新幅度，過大時無法收斂，過小時訓練速度慢、可能陷入局部最小值無法進步。
            - 批次大小：每次投入訓練的數量，與硬體GPU的記憶體有關。批次大的穩定度高，但需要消耗更多資源。
            - 優化算法：影響梯度訓練時的參數更新狀態。以預設值即可。
                - Adam：學習率的自適應調整，收斂較快，不同模型的泛用性高。
                - SGD：計算簡單，對資料集大的訓練較友善，但收斂較慢，容易陷入局部最小值無法進步。
        4. 訓練與驗證模型：準確率、損失函數、mAP，避免過擬合或精度不足
            - 混淆矩陣：predicted label、true label，TP、TN、FP偽陽、FN偽陰
                - 準確率(accuracy)：(TP+TN) / All
                - 精確度(precision) = TP / (TP+FP)。用於錯誤預測代價高的情境，如垃圾郵件過濾。
                - 召回率(recall) = TP / (TP+FN)。用於漏檢代價高的情境，如疾病診斷。
                - F1分數(F1 score) = 2 x 精確度 x 召回率 / (精確度 + 召回率)。是調和平均數，用於評估模型。
            - 精度指標
                - 交併比(IoU) = 預測框與真實框的交集區域 / 聯集區域。用於物件偵測的評估。
                - 平均精度(AP)：單一類別中，不同召回率的精確度平均值。用於「單一物件」類別的評估。
                - 平均精確度(mAP)：不同IoU閥值的AP平均值。用於不同物件類別下「模型性能」的評估。
        5. 模型部署：實際使用已訓練的模型，可整合至目前系統。
            - 模型轉化：考慮機台的效能，轉換成輕量化，可用邊緣裝置(手機、智能相機Jeston Nano)。如TensorFlow的SavedModel或ONNX。
            - 平台選擇：使用雲端硬件推理，降低部署成本的伺服器、硬碟購買。
                - 雲端AWS、雲端GCP。使用雲端硬件推理，降低部署成本的伺服器、硬碟購買。
            - 設定存取點API
            - 測試部署
            - 範例：使用BailAI平台，將模型.pt進行模型轉換，變成通用的格式.onnx，再轉換成不同邊緣裝置需要的配置檔格式如.engine、.tlt、.wts。再使用DeepStream對邊緣裝置部署該模型，開始實時影像分析(webcam或智能相機)，並將檢測及回饋，結合到目前系統發警報、發通報。